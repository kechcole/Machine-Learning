{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST REGRESSOR.\n",
    "\n",
    "Random forest algorithm is a supervised machine learning technique that uses decision tree as its base for prediction. It ensembles multiple decision trees together and aggregates/votes values to estimate the values. When training each estimator, it introduces random trees that run parallel and do not interact with each other during tree building. This model can perform both classification and regression but we are going to test the latter. \n",
    "\n",
    "By combining multiple models, this algorithm achieves higher prediction accuracy and optimisation by reducing variance across models. Random forest uses bagging technique(ensemble type) where multiple models run independently in parrallel then their values averged to give an outcome. Boosting on the other end learns sequentially and corrects errors at each step to imrove stability of a model. It adapts to the model.  \n",
    "\n",
    "Random forest regressor learns by building many decision tress, mostly 100's, then finds the average regression of all trees. Original data is brocken into small subsets (tuples) iteratively, in each, an observation is selected through row sampling with replacement method (boostraping). A model is then created and learns from each training sample and then returns an outcome. The model then counts all values and the one having most occurance is assigned to an unknown value X, this is bagging.  Individual decision trees have high varaiance, but when they are combined in parrallel then the resulting variance is greatly reduced since each model is trained perfectly on different  sample data. Accuracy is higher becaused a decision is pegged on multiple models (no dependence on a single model). \n",
    "\n",
    "Random Forest is generally more accurate and robust than regression. It is also less prone to overfitting because it emphasises on the element of randomness which means that it is more likely to generalize well to new data. This also ensures that the tree are NOT correlated as much as possible. \n",
    "\n",
    "Random forests is designed to handle shortfalls from decision tree, this includes ;\n",
    "\n",
    "- Decision trees are prone to overfitting because of high variances in single models. With random forests, variance is reduced by running and ensemble of decisions tree models.\n",
    "- Decision tree model is highly sensitive, minor changes in the original data can give a completly different prediction making them very unstable.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Like any other machine learning process, the steps are as follows ;\n",
    "\n",
    "1. Define a problem - predict US election voter turnout for the 2020.\n",
    "2. Access data in the appropriate format for python language \n",
    "3. Study the data to correct anomalies such as missing values that may hinder you from achieving the objective.\n",
    "4. Create a machine learning model, random forest regressor will be used.\n",
    "5. Train the model to understand relationship betweem predictor and target variables. \n",
    "6. Test your model and evaluate performance. \n",
    "7. Refine your model to suite your objectives - get more data, use a different technique, add parameters.\n",
    "8. Finally interpret the data gained and report as needed. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
